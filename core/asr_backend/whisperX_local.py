# ------------
# æœ¬æ¨¡å—ç”¨äºæœ¬åœ°WhisperXæ¨¡å‹çš„åŠ è½½ã€æ¨ç†å’Œå¯¹é½ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©HuggingFaceé•œåƒ
# ------------
import os
import warnings
import time
import subprocess
import torch
import whisperx
import librosa
from rich import print as rprint
from core.utils import *

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")
# æ¨¡å‹å­˜å‚¨ç›®å½•
MODEL_DIR = load_key("model_dir")

# ------------
# æ£€æŸ¥HuggingFaceé•œåƒé€Ÿåº¦ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„é•œåƒ
# ------------
@except_handler("failed to check hf mirror", default_return=None)
def check_hf_mirror():
    mirrors = {'Official': 'huggingface.co', 'Mirror': 'hf-mirror.com'}
    fastest_url = f"https://{mirrors['Official']}"
    best_time = float('inf')
    rprint("[cyan]ğŸ” æ­£åœ¨æ£€æµ‹HuggingFaceé•œåƒ...[/cyan]")
    for name, domain in mirrors.items():
        if os.name == 'nt':
            cmd = ['ping', '-n', '1', '-w', '3000', domain]
        else:
            cmd = ['ping', '-c', '1', '-W', '3', domain]
        start = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True)
        response_time = time.time() - start
        if result.returncode == 0:
            if response_time < best_time:
                best_time = response_time
                fastest_url = f"https://{domain}"
            rprint(f"[green]âœ“ {name}:[/green] {response_time:.2f}s")
    if best_time == float('inf'):
        rprint("[yellow]âš ï¸ æ‰€æœ‰é•œåƒæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤[/yellow]")
    rprint(f"[cyan]ğŸš€ é€‰ç”¨é•œåƒ:[/cyan] {fastest_url} ({best_time:.2f}s)")
    return fastest_url

# ------------
# æœ¬åœ°WhisperXæ¨ç†ä¸å¯¹é½ä¸»æµç¨‹
# ------------
@except_handler("WhisperX processing error:")
def transcribe_audio(raw_audio_file, vocal_audio_file, start, end):
    # è®¾ç½®HuggingFaceé•œåƒ
    os.environ['HF_ENDPOINT'] = check_hf_mirror()
    WHISPER_LANGUAGE = load_key("whisper.language")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rprint(f"ğŸš€ å¯åŠ¨WhisperXï¼Œè®¾å¤‡: {device} ...")
    # æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è°ƒæ•´batch sizeå’Œè®¡ç®—ç±»å‹
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        batch_size = 16 if gpu_mem > 8 else 2
        compute_type = "float16" if torch.cuda.is_bf16_supported() else "int8"
        rprint(f"[cyan]ğŸ® GPUæ˜¾å­˜:[/cyan] {gpu_mem:.2f} GB, [cyan]ğŸ“¦ Batch size:[/cyan] {batch_size}, [cyan]âš™ï¸ è®¡ç®—ç±»å‹:[/cyan] {compute_type}")
    else:
        batch_size = 1
        compute_type = "int8"
        rprint(f"[cyan]ğŸ“¦ Batch size:[/cyan] {batch_size}, [cyan]âš™ï¸ è®¡ç®—ç±»å‹:[/cyan] {compute_type}")
    rprint(f"[green]â–¶ï¸ å¼€å§‹WhisperXè½¬å½• {start:.2f}s åˆ° {end:.2f}s...[/green]")
    # é€‰æ‹©æ¨¡å‹
    if WHISPER_LANGUAGE == 'zh':
        model_name = "Huan69/Belle-whisper-large-v3-zh-punct-fasterwhisper"
        local_model = os.path.join(MODEL_DIR, "Belle-whisper-large-v3-zh-punct-fasterwhisper")
    else:
        model_name = load_key("whisper.model")
        local_model = os.path.join(MODEL_DIR, model_name)
    if os.path.exists(local_model):
        rprint(f"[green]ğŸ“¥ åŠ è½½æœ¬åœ°WHISPERæ¨¡å‹:[/green] {local_model} ...")
        model_name = local_model
    else:
        rprint(f"[green]ğŸ“¥ ä½¿ç”¨HuggingFaceæ¨¡å‹:[/green] {model_name} ...")
    # VADå’ŒASRå‚æ•°
    vad_options = {"vad_onset": 0.500,"vad_offset": 0.363}
    asr_options = {"temperatures": [0],"initial_prompt": "",}
    whisper_language = None if 'auto' in WHISPER_LANGUAGE else WHISPER_LANGUAGE
    rprint("[bold yellow] å¯å¿½ç•¥`Model was trained with torch...`è­¦å‘Š[/bold yellow]")
    # åŠ è½½æ¨¡å‹
    model = whisperx.load_model(model_name, device, compute_type=compute_type, language=whisper_language, vad_options=vad_options, asr_options=asr_options, download_root=MODEL_DIR)
    # åŠ è½½éŸ³é¢‘ç‰‡æ®µ
    def load_audio_segment(audio_file, start, end):
        audio, _ = librosa.load(audio_file, sr=16000, offset=start, duration=end - start, mono=True)
        return audio
    raw_audio_segment = load_audio_segment(raw_audio_file, start, end)
    vocal_audio_segment = load_audio_segment(vocal_audio_file, start, end)
    # ------------
    # 1. è½¬å½•åŸå§‹éŸ³é¢‘
    # ------------
    transcribe_start_time = time.time()
    rprint("[bold green]æç¤º: è‹¥æ­£å¸¸å·¥ä½œä¼šæ˜¾ç¤ºè¿›åº¦æ¡â†“[/bold green]")
    result = model.transcribe(raw_audio_segment, batch_size=batch_size, print_progress=True)
    transcribe_time = time.time() - transcribe_start_time
    rprint(f"[cyan]â±ï¸ è½¬å½•ç”¨æ—¶:[/cyan] {transcribe_time:.2f}s")
    # é‡Šæ”¾GPUèµ„æº
    del model
    torch.cuda.empty_cache()
    # ä¿å­˜è¯­è¨€
    update_key("whisper.language", result['language'])
    if result['language'] == 'zh' and WHISPER_LANGUAGE != 'zh':
        raise ValueError("è¯·æŒ‡å®šè½¬å½•è¯­è¨€ä¸ºzhåé‡è¯•ï¼")
    # ------------
    # 2. ç”¨äººå£°éŸ³é¢‘å¯¹é½æ—¶é—´æˆ³
    # ------------
    align_start_time = time.time()
    model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
    result = whisperx.align(result["segments"], model_a, metadata, vocal_audio_segment, device, return_char_alignments=False)
    align_time = time.time() - align_start_time
    rprint(f"[cyan]â±ï¸ å¯¹é½ç”¨æ—¶:[/cyan] {align_time:.2f}s")
    # å†æ¬¡é‡Šæ”¾GPUèµ„æº
    torch.cuda.empty_cache()
    del model_a
    # æ—¶é—´æˆ³å…¨å±€åç§»
    for segment in result['segments']:
        segment['start'] += start
        segment['end'] += start
        for word in segment['words']:
            if 'start' in word:
                word['start'] += start
            if 'end' in word:
                word['end'] += start
    return result