# -*- coding: utf-8 -*-
"""
æ™ºèƒ½é€—å·åˆ†å‰²æ¨¡å—

æœ¬æ¨¡å—æ˜¯å¥å­åˆ†å‰²æµç¨‹ä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œä¸“é—¨è´Ÿè´£å¤„ç†åŸºäºé€—å·çš„æ™ºèƒ½åˆ†å‰²ã€‚
ä¸ç®€å•çš„æŒ‰é€—å·åˆ‡åˆ†ä¸åŒï¼Œæœ¬æ¨¡å—åˆ©ç”¨ spaCy è¿›è¡Œå¥æ³•åˆ†æï¼Œåˆ¤æ–­é€—å·æ˜¯å¦åˆ†éš”äº†ä¸¤ä¸ª
åœ¨è¯­æ³•ä¸Šç›¸å¯¹ç‹¬ç«‹çš„å­å¥ï¼ˆå³éƒ½åŒ…å«ä¸»è¯­å’Œè°“è¯­ï¼‰ï¼Œä»è€Œå†³å®šæ˜¯å¦è¿›è¡Œåˆ†å‰²ã€‚

æ ¸å¿ƒé€»è¾‘:
1.  **æœ‰æ•ˆçŸ­è¯­åˆ¤æ–­**: `is_valid_phrase` å‡½æ•°é€šè¿‡æ£€æŸ¥ä¸€ä¸ª spaCy çŸ­è¯­ç‰‡æ®µï¼ˆSpanï¼‰æ˜¯å¦åŒæ—¶
    åŒ…å«ä¸»è¯­ï¼ˆnsubj, nsubjpassï¼‰å’ŒåŠ¨è¯ï¼ˆVERB, AUXï¼‰æ¥åˆ¤æ–­å…¶æ˜¯å¦æ„æˆä¸€ä¸ªæœ‰æ•ˆçš„å­å¥ã€‚
2.  **é€—å·ä¸Šä¸‹æ–‡åˆ†æ**: `analyze_comma` å‡½æ•°æ˜¯å†³ç­–æ ¸å¿ƒã€‚å¯¹äºæ¯ä¸€ä¸ªé€—å·ï¼Œå®ƒä¼šæ£€æŸ¥å…¶å³ä¾§çš„
    çŸ­è¯­æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å­å¥ï¼Œå¹¶ç¡®ä¿é€—å·ä¸¤ä¾§éƒ½æœ‰è¶³å¤Ÿæ•°é‡çš„è¯ï¼ˆé»˜è®¤å¤§äº3ï¼‰ï¼Œä»¥é¿å…
    åœ¨åˆ—è¡¨æˆ–ç®€çŸ­æ’å…¥è¯­ä¸­è¿›è¡Œä¸å½“åˆ†å‰²ã€‚
3.  **æµæ°´çº¿æ“ä½œ**: `split_by_comma_main` å‡½æ•°å®šä¹‰äº†æœ¬æ¨¡å—çš„å·¥ä½œæµã€‚å®ƒä¼šè¯»å–ä¸Šä¸€æ­¥
    ï¼ˆæŒ‰æ ‡ç‚¹åˆ†å‰²ï¼‰çš„è¾“å‡ºæ–‡ä»¶ï¼Œå¯¹å…¶ä¸­çš„æ¯ä¸€å¥è¯åº”ç”¨é€—å·åˆ†å‰²é€»è¾‘ï¼Œç„¶åå°†ç»“æœå†™å…¥æ–°çš„
    è¾“å‡ºæ–‡ä»¶ï¼Œå¹¶åˆ é™¤åŸå§‹çš„è¾“å…¥æ–‡ä»¶ã€‚

è¿™ç§æ™ºèƒ½åˆ†å‰²æ–¹æ³•å¯ä»¥æœ‰æ•ˆå¤„ç†å¹¶åˆ—å¥ã€å¤åˆå¥ï¼ŒåŒæ—¶é¿å…ç ´åå¥å­ç»“æ„ï¼Œæé«˜åˆ†å‰²è´¨é‡ã€‚
"""

import itertools
import os
import warnings
from core.utils import rprint, write_to_file
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_COMMA_FILE, SPLIT_BY_MARK_FILE

# å¿½ç•¥æ¥è‡ªæ—§ç‰ˆæœ¬åº“å¯èƒ½äº§ç”Ÿçš„FutureWarningè­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning)

def is_valid_phrase(phrase) -> bool:
    """
    æ£€æŸ¥ä¸€ä¸ªçŸ­è¯­ï¼ˆspaCy Spanå¯¹è±¡ï¼‰æ˜¯å¦æ„æˆä¸€ä¸ªæœ‰æ•ˆçš„ç‹¬ç«‹å­å¥ã€‚
    æœ‰æ•ˆå­å¥çš„æ ‡å‡†æ˜¯ï¼šå¿…é¡»åŒæ—¶åŒ…å«ä¸»è¯­å’ŒåŠ¨è¯ã€‚

    Args:
        phrase (spacy.tokens.span.Span): éœ€è¦åˆ†æçš„çŸ­è¯­ã€‚

    Returns:
        bool: å¦‚æœçŸ­è¯­åŒ…å«ä¸»è¯­å’ŒåŠ¨è¯ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸»è¯­ï¼šä¾å­˜å…³ç³»ä¸º'nsubj'(åè¯ä¸»è¯­)æˆ–'nsubjpass'(è¢«åŠ¨åè¯ä¸»è¯­)ï¼Œæˆ–è€…è¯æ€§ä¸ºä»£è¯(PRON)ã€‚
    has_subject = any(token.dep_ in ["nsubj", "nsubjpass"] or token.pos_ == "PRON" for token in phrase)
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŠ¨è¯ï¼šè¯æ€§ä¸º'VERB'(åŠ¨è¯)æˆ–'AUX'(åŠ©åŠ¨è¯)ã€‚
    has_verb = any((token.pos_ == "VERB" or token.pos_ == 'AUX') for token in phrase)
    return has_subject and has_verb

def analyze_comma(start: int, doc, token) -> bool:
    """
    åˆ†æç»™å®šé€—å·æ˜¯å¦é€‚åˆä½œä¸ºåˆ†å‰²ç‚¹ã€‚

    Args:
        start (int): å½“å‰å¤„ç†å¥æ®µåœ¨åŸå§‹docä¸­çš„èµ·å§‹ç´¢å¼•ã€‚
        doc (spacy.tokens.doc.Doc): å®Œæ•´çš„spaCy Docå¯¹è±¡ã€‚
        token (spacy.tokens.token.Token): å½“å‰æ­£åœ¨åˆ†æçš„é€—å·tokenã€‚

    Returns:
        bool: å¦‚æœè¯¥é€—å·é€‚åˆåˆ†å‰²ï¼Œè¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    # æå–é€—å·å·¦ä¾§å’Œå³ä¾§çš„ä¸Šä¸‹æ–‡çŸ­è¯­ï¼Œçª—å£å¤§å°çº¦ä¸º9ä¸ªtokenã€‚
    left_phrase = doc[max(start, token.i - 9):token.i]
    right_phrase = doc[token.i + 1:min(len(doc), token.i + 10)]
    
    # æ ¸å¿ƒåˆ¤æ–­ï¼šåªæœ‰å½“é€—å·å³ä¾§çš„çŸ­è¯­æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç‹¬ç«‹å­å¥æ—¶ï¼Œæ‰è€ƒè™‘åˆ†å‰²ã€‚
    # å·¦ä¾§çŸ­è¯­é»˜è®¤è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒæ˜¯å¥å­çš„å‰åŠéƒ¨åˆ†ã€‚
    suitable_for_splitting = is_valid_phrase(right_phrase)
    
    # é™„åŠ æ¡ä»¶ï¼šä¸ºé˜²æ­¢åœ¨çŸ­åˆ—è¡¨æˆ–æ’å…¥è¯­ï¼ˆå¦‚ 'a, b, c'ï¼‰ä¸­åˆ†å‰²ï¼Œ
    # æ£€æŸ¥é€—å·ä¸¤ä¾§çš„å®é™…å•è¯æ•°é‡ï¼ˆæ’é™¤æ ‡ç‚¹ç¬¦å·ï¼‰ã€‚
    left_words = [t for t in left_phrase if not t.is_punct]
    # å¯¹äºå³ä¾§ï¼Œåªæ£€æŸ¥åˆ°ä¸‹ä¸€ä¸ªæ ‡ç‚¹å‰çš„å•è¯ï¼Œé¿å…è·¨è¶Šå¤šä¸ªå­å¥ã€‚
    right_words = list(itertools.takewhile(lambda t: not t.is_punct, right_phrase))
    
    # å¦‚æœä»»ä½•ä¸€ä¾§çš„å•è¯æ•°å°äºç­‰äº3ï¼Œåˆ™è®¤ä¸ºä¸é€‚åˆåˆ†å‰²ã€‚
    if len(left_words) <= 3 or len(right_words) <= 3:
        suitable_for_splitting = False

    return suitable_for_splitting

def split_by_comma(text: str, nlp) -> list:
    """
    å¯¹å•å¥æ–‡æœ¬åº”ç”¨æ™ºèƒ½é€—å·åˆ†å‰²é€»è¾‘ã€‚

    Args:
        text (str): éœ€è¦åˆ†å‰²çš„å•å¥æ–‡æœ¬ã€‚
        nlp (spacy.Language): å·²åŠ è½½çš„spaCyæ¨¡å‹å®ä¾‹ã€‚

    Returns:
        list: åˆ†å‰²åçš„å­å¥åˆ—è¡¨ã€‚
    """
    doc = nlp(text)
    sentences = []
    start = 0  # å½“å‰å­å¥çš„èµ·å§‹tokenç´¢å¼•
    
    # éå†å¥å­ä¸­çš„æ¯ä¸€ä¸ªtoken
    for token in doc:
        # åŒæ—¶æ£€æŸ¥è‹±æ–‡é€—å·å’Œä¸­æ–‡é€—å·
        if token.text in [",", "ï¼Œ"]:
            # è°ƒç”¨åˆ†æå‡½æ•°åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨æ­¤å¤„åˆ†å‰²
            if analyze_comma(start, doc, token):
                # å¦‚æœé€‚åˆåˆ†å‰²ï¼Œåˆ™å°†é€—å·å‰çš„éƒ¨åˆ†ä½œä¸ºä¸€ä¸ªå­å¥æ·»åŠ 
                sentences.append(doc[start:token.i].text.strip())
                rprint(f"[yellow]âœ‚ï¸  åœ¨é€—å·å¤„åˆ†å‰²: ...{doc[start:token.i].text[-10:]},| {doc[token.i + 1:].text[:10]}...[/yellow]")
                # æ›´æ–°ä¸‹ä¸€ä¸ªå­å¥çš„èµ·å§‹ä½ç½®åˆ°é€—å·ä¹‹å
                start = token.i + 1
    
    # å°†æœ€åä¸€ä¸ªåˆ†å‰²ç‚¹åˆ°å¥å­æœ«å°¾çš„éƒ¨åˆ†ä½œä¸ºæœ€åä¸€ä¸ªå­å¥æ·»åŠ 
    sentences.append(doc[start:].text.strip())
    return sentences

def split_by_comma_main(nlp):
    """
    æ‰§è¡Œé€—å·åˆ†å‰²çš„ä¸»å‡½æ•°ï¼Œæ˜¯æ•´ä¸ªæµç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
    å®ƒè¯»å–å‰ä¸€é˜¶æ®µï¼ˆæŒ‰æ ‡ç‚¹åˆ†å‰²ï¼‰çš„è¾“å‡ºï¼Œè¿›è¡Œé€—å·åˆ†å‰²ï¼Œç„¶åå°†ç»“æœå†™å…¥æ–°æ–‡ä»¶ï¼Œ
    å¹¶æ¸…ç†æ‰å‰ä¸€é˜¶æ®µçš„æ–‡ä»¶ã€‚

    Args:
        nlp (spacy.Language): å·²åŠ è½½çš„spaCyæ¨¡å‹å®ä¾‹ã€‚
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç›´æ¥è¿”å›ï¼Œé¿å…é”™è¯¯ã€‚
    if not os.path.exists(SPLIT_BY_MARK_FILE):
        rprint(f"[yellow]æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {SPLIT_BY_MARK_FILE}ï¼Œè·³è¿‡é€—å·åˆ†å‰²æ­¥éª¤ã€‚[/yellow]")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„è¾“å‡ºæ–‡ä»¶ï¼Œä»¥ç¡®ä¿åç»­æµç¨‹çš„è¿è´¯æ€§
        write_to_file(SPLIT_BY_COMMA_FILE, "")
        return

    with open(SPLIT_BY_MARK_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()

    all_split_sentences = []
    for sentence in sentences:
        # å¯¹æ¯ä¸€å¥ä»æ–‡ä»¶ä¸­è¯»å‡ºçš„å¥å­ï¼Œåº”ç”¨é€—å·åˆ†å‰²é€»è¾‘
        split_sentences = split_by_comma(sentence.strip(), nlp)
        all_split_sentences.extend(split_sentences)

    # å°†æ‰€æœ‰åˆ†å‰²åçš„å­å¥å†™å…¥è¾“å‡ºæ–‡ä»¶
    output_content = "\n".join(filter(None, all_split_sentences))
    write_to_file(SPLIT_BY_COMMA_FILE, output_content)
    
    # é‡è¦ï¼šåˆ é™¤åŸå§‹çš„è¾“å…¥æ–‡ä»¶ï¼Œå› ä¸ºå®ƒå·²ç»è¢«å¤„ç†å¹¶è¢«æ–°æ–‡ä»¶å–ä»£ã€‚
    # è¿™æ˜¯æµæ°´çº¿è®¾è®¡çš„ä¸€éƒ¨åˆ†ã€‚
    os.remove(SPLIT_BY_MARK_FILE)
    
    rprint(f"[green]ğŸ’¾ æŒ‰é€—å·åˆ†å‰²å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° â†’ `{SPLIT_BY_COMMA_FILE}`[/green]")

if __name__ == "__main__":
    # --- æµ‹è¯•ä»£ç  ---
    # åˆå§‹åŒ–NLPæ¨¡å‹
    test_nlp = init_nlp()
    # æ„é€ ä¸€ä¸ªæµ‹è¯•ç”¨çš„è¾“å…¥æ–‡ä»¶
    test_sentences = [
        "So in the same frame, right there, almost in the exact same spot on the ice, Brown has committed himself, whereas McDavid has not.",
        "This is a simple sentence, it has two parts.",
        "We need to buy apples, oranges, and bananas."
    ]
    with open(SPLIT_BY_MARK_FILE, "w", encoding="utf-8") as f:
        for s in test_sentences:
            f.write(s + "\n")
    
    # è¿è¡Œä¸»å‡½æ•°è¿›è¡Œæµ‹è¯•
    split_by_comma_main(test_nlp)
    
    # æ‰“å°ç»“æœè¿›è¡ŒéªŒè¯
    with open(SPLIT_BY_COMMA_FILE, "r", encoding="utf-8") as f:
        rprint("\n--- åˆ†å‰²ç»“æœ ---")
        print(f.read())
        rprint("--- æµ‹è¯•ç»“æŸ ---")