# -*- coding: utf-8 -*-
"""
åŸºäºè¿æ¥è¯çš„æ™ºèƒ½å¥å­åˆ†å‰²æ¨¡å—

æœ¬æ¨¡å—æ˜¯å¥å­åˆ†å‰²æµæ°´çº¿ä¸­çš„é«˜çº§æ­¥éª¤ï¼Œä¸“é—¨ç”¨äºå¤„ç†åŸºäºè¯­æ³•è¿æ¥è¯çš„åˆ†å‰²ã€‚
å®ƒæ”¯æŒå¤šç§è¯­è¨€ï¼Œå¹¶åˆ©ç”¨ spaCy æä¾›çš„å¥æ³•ä¾å­˜åˆ†ææ¥æ™ºèƒ½åœ°åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨è¿æ¥è¯å¤„åˆ‡åˆ†å¥å­ï¼Œ
ä»è€Œæœ‰æ•ˆå¤„ç†å¤æ‚çš„å¤åˆå¥å’Œå¹¶åˆ—å¥ã€‚

æ ¸å¿ƒåŠŸèƒ½:
- **å¤šè¯­è¨€æ”¯æŒ**: å†…ç½®äº†å¯¹è‹±è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ã€æ³•è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­å’Œæ„å¤§åˆ©è¯­ç­‰å¤šç§è¯­è¨€çš„
  è¿æ¥è¯å’Œè¯­æ³•è§„åˆ™çš„æ”¯æŒã€‚
- **å¥æ³•æ„ŸçŸ¥åˆ†å‰²**: ä¸ä»…ä»…æ˜¯æŸ¥æ‰¾å…³é”®è¯ï¼Œ`analyze_connectors` å‡½æ•°ä¼šåˆ†æè¿æ¥è¯çš„è¯æ€§ï¼ˆPOSï¼‰å’Œ
  ä¾å­˜å…³ç³»ï¼ˆDependencyï¼‰ï¼Œä»¥åŒºåˆ†å…¶åœ¨å¥å­ä¸­çš„ä¸åŒè¯­æ³•åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥åŒºåˆ†ä½œä¸ºä»å¥å¼•å¯¼è¯çš„ 'that'
  å’Œä½œä¸ºé™å®šè¯çš„ 'that'ï¼Œåªåœ¨å‰è€…çš„æƒ…å†µä¸‹è¿›è¡Œåˆ†å‰²ã€‚
- **è¿­ä»£å¼åˆ†å‰²**: `split_by_connectors` å‡½æ•°é‡‡ç”¨ä¸€ç§å·§å¦™çš„è¿­ä»£æ–¹æ³•ï¼Œæ¯æ¬¡åªåœ¨å¥å­ä¸­ç¬¬ä¸€ä¸ª
  æœ€åˆé€‚çš„è¿æ¥è¯å¤„è¿›è¡Œåˆ†å‰²ï¼Œç„¶ååœ¨æ–°ç”Ÿæˆçš„å¥å­ç‰‡æ®µä¸Šé‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šçš„åˆ†å‰²ç‚¹ã€‚
  è¿™ç§æ–¹æ³•å¯ä»¥é¿å…å°†ä¸€ä¸ªé•¿å¥ä¸€æ¬¡æ€§åˆ‡å¾—è¿‡äºé›¶ç¢ã€‚
- **æµæ°´çº¿æ“ä½œ**: ä½œä¸ºå¤„ç†æµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œ`split_sentences_main` å‡½æ•°ä¼šè¯»å–ä¸Šä¸€æ­¥ï¼ˆé€—å·åˆ†å‰²ï¼‰
  çš„è¾“å‡ºï¼Œåº”ç”¨æœ¬æ¨¡å—çš„é€»è¾‘ï¼Œå°†ç»“æœå†™å…¥æ–°æ–‡ä»¶ï¼Œå¹¶åˆ é™¤ä¸Šä¸€æ­¥çš„ä¸­é—´æ–‡ä»¶ã€‚
"""

import os
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_COMMA_FILE, SPLIT_BY_CONNECTOR_FILE
from core.utils import rprint, write_to_file

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=FutureWarning)

def analyze_connectors(doc, token) -> bool:
    """
    åˆ†æä¸€ä¸ª token æ˜¯å¦ä¸ºåº”è§¦å‘å¥å­åˆ†å‰²çš„è¿æ¥è¯ã€‚
    è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒå†³ç­–å‡½æ•°ï¼Œå®ƒåŸºäºè¯­è¨€å’Œè¯çš„å¥æ³•åŠŸèƒ½æ¥åˆ¤æ–­ã€‚

    å¤„ç†é€»è¾‘é¡ºåº:
     1. æ ¹æ®æ–‡æ¡£è¯­è¨€ï¼Œç¡®å®šè¦æ£€æŸ¥çš„è¿æ¥è¯åˆ—è¡¨å’Œç›¸åº”çš„è¯­æ³•è§„åˆ™ã€‚
     2. æ£€æŸ¥å½“å‰ token æ˜¯å¦åœ¨ç›®æ ‡è¿æ¥è¯åˆ—è¡¨ä¸­ã€‚
     3. å¯¹ç‰¹å®šè¯ï¼ˆå¦‚è‹±è¯­ä¸­çš„ 'that'ï¼‰è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œæ£€æŸ¥å…¶ä¾å­˜å…³ç³»ä»¥ç¡®å®šå…¶åŠŸèƒ½ã€‚
     4. æ£€æŸ¥ token æ˜¯å¦ä½œä¸ºåè¯çš„é™å®šè¯æˆ–ä»£è¯ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä¸åˆ†å‰²ã€‚
     5. å¦‚æœä»¥ä¸Šæ¡ä»¶éƒ½ä¸æ»¡è¶³ï¼Œé»˜è®¤å¯¹äºå¤§éƒ¨åˆ†è¿æ¥è¯è¿›è¡Œåˆ†å‰²ã€‚

    Args:
        doc (spacy.tokens.doc.Doc): å½“å‰æ­£åœ¨å¤„ç†çš„ spaCy Doc å¯¹è±¡ã€‚
        token (spacy.tokens.token.Token): éœ€è¦åˆ†æçš„ tokenã€‚

    Returns:
        bool: å¦‚æœåº”è¯¥åœ¨è¯¥ token å‰è¿›è¡Œåˆ†å‰²ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
    """
    lang = doc.lang_  # è·å–æ–‡æ¡£çš„è¯­è¨€ä»£ç 

    # --- å¤šè¯­è¨€è¿æ¥è¯å’Œè¯­æ³•è§„åˆ™å®šä¹‰ ---
    # ä¸ºæ¯ç§æ”¯æŒçš„è¯­è¨€å®šä¹‰è¿æ¥è¯åˆ—è¡¨å’Œç›¸å…³çš„ä¾å­˜å…³ç³»ã€è¯æ€§æ ‡ç­¾ã€‚
    if lang == "en":
        connectors = ["that", "which", "where", "when", "because", "but", "and", "or"]
        mark_dep = "mark"  # ä»å¥å¼•å¯¼è¯çš„ä¾å­˜å…³ç³»
        det_pron_deps = ["det", "pron"]  # é™å®šè¯å’Œä»£è¯çš„ä¾å­˜å…³ç³»
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "zh":
        connectors = ["å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "è€Œä¸”", "è™½ç„¶", "å¦‚æœ", "å³ä½¿", "å°½ç®¡"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    # ... (å…¶ä»–è¯­è¨€çš„å®šä¹‰ä¸æ­¤ç±»ä¼¼) ...
    elif lang == "ja":
        connectors = ["ã‘ã‚Œã©ã‚‚", "ã—ã‹ã—", "ã ã‹ã‚‰", "ãã‚Œã§", "ã®ã§", "ã®ã«", "ãŸã‚"]
        mark_dep = "mark"
        det_pron_deps = ["case"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "fr":
        connectors = ["que", "qui", "oÃ¹", "quand", "parce que", "mais", "et", "ou"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ru":
        connectors = ["Ñ‡Ñ‚Ğ¾", "ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹", "Ğ³Ğ´Ğµ", "ĞºĞ¾Ğ³Ğ´Ğ°", "Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾", "Ğ½Ğ¾", "Ğ¸", "Ğ¸Ğ»Ğ¸"]
        mark_dep = "mark"
        det_pron_deps = ["det"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "es":
        connectors = ["que", "cual", "donde", "cuando", "porque", "pero", "y", "o"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "de":
        connectors = ["dass", "welche", "wo", "wann", "weil", "aber", "und", "oder"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "it":
        connectors = ["che", "quale", "dove", "quando", "perchÃ©", "ma", "e", "o"]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    else:
        # å¦‚æœè¯­è¨€ä¸æ”¯æŒï¼Œåˆ™ä¸è¿›è¡Œåˆ†å‰²
        return False
    
    # å¦‚æœå½“å‰ token ä¸åœ¨è¿æ¥è¯åˆ—è¡¨ä¸­ï¼Œåˆ™ä¸åˆ†å‰²
    if token.text.lower() not in connectors:
        return False
    
    # --- ç‰¹å®šè§„åˆ™åˆ¤æ–­ ---
    # è§„åˆ™1: å¯¹è‹±è¯­ä¸­çš„ 'that' è¿›è¡Œç‰¹æ®Šå¤„ç†
    if lang == "en" and token.text.lower() == "that":
        # å¦‚æœ 'that' æ˜¯ä¸€ä¸ªä»å¥å¼•å¯¼è¯ (mark)ï¼Œå¹¶ä¸”å…¶å¤´éƒ¨æ˜¯ä¸€ä¸ªåŠ¨è¯ï¼Œé‚£ä¹ˆå®ƒå¾ˆå¯èƒ½å¼•å¯¼ä¸€ä¸ªæ–°å­å¥ï¼Œåº”è¯¥åˆ†å‰²ã€‚
        if token.dep_ == mark_dep and token.head.pos_ == verb_pos:
            return True
        else:
            # å¦åˆ™ï¼Œ'that' å¯èƒ½æ˜¯ä¸€ä¸ªé™å®šè¯ï¼ˆå¦‚ 'that book'ï¼‰ï¼Œä¸åº”åˆ†å‰²ã€‚
            return False
    # è§„åˆ™2: å¦‚æœ token æ˜¯ä¸€ä¸ªé™å®šè¯æˆ–ä»£è¯ï¼Œå¹¶ä¸”å…¶å¤´éƒ¨æ˜¯ä¸€ä¸ªåè¯ï¼Œåˆ™ä¸åˆ†å‰²ã€‚
    # è¿™å¯ä»¥é˜²æ­¢åœ¨å…³ç³»ä»å¥ä¸­ï¼ˆå¦‚ 'the man who...'ï¼‰æˆ–åè¯çŸ­è¯­ä¸­é”™è¯¯åœ°åˆ†å‰²ã€‚
    elif token.dep_ in det_pron_deps and token.head.pos_ in noun_pos:
        return False
    else:
        # é»˜è®¤æƒ…å†µï¼šå¦‚æœ token æ˜¯ä¸€ä¸ªè¿æ¥è¯ä¸”ä¸æ»¡è¶³ä¸Šè¿°æ’é™¤æ¡ä»¶ï¼Œåˆ™è¿›è¡Œåˆ†å‰²ã€‚
        return True

def split_by_connectors(text: str, context_words: int = 5, nlp=None) -> list:
    """
    å¯¹å•å¥æ–‡æœ¬åº”ç”¨åŸºäºè¿æ¥è¯çš„è¿­ä»£å¼åˆ†å‰²é€»è¾‘ã€‚

    Args:
        text (str): éœ€è¦åˆ†å‰²çš„å•å¥æ–‡æœ¬ã€‚
        context_words (int): åˆ†å‰²ç‚¹ä¸¤ä¾§å¿…é¡»æ»¡è¶³çš„æœ€å°å•è¯æ•°ï¼Œç”¨äºé¿å…åœ¨çŸ­è¯­ä¸­åˆ†å‰²ã€‚
        nlp (spacy.Language): å·²åŠ è½½çš„ spaCy æ¨¡å‹å®ä¾‹ã€‚

    Returns:
        list: åˆ†å‰²åçš„å­å¥åˆ—è¡¨ã€‚
    """
    doc = nlp(text)
    sentences = [doc.text]  # åˆå§‹æ—¶ï¼Œå¥å­åˆ—è¡¨åªåŒ…å«åŸå§‹å¥å­
    
    # è¿­ä»£åˆ†å‰²ï¼Œç›´åˆ°å¥å­ä¸­å†ä¹Ÿæ‰¾ä¸åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹
    while True:
        split_occurred = False  # æ ‡è®°æœ¬è½®è¿­ä»£æ˜¯å¦å‘ç”Ÿäº†åˆ†å‰²
        new_sentences = []
        
        for sent in sentences:
            doc = nlp(sent)
            start = 0
            
            # éå†å¥å­ä¸­çš„æ¯ä¸ª tokenï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªåˆé€‚çš„åˆ†å‰²ç‚¹
            for i, token in enumerate(doc):
                should_split = analyze_connectors(doc, token)
                
                # è·³è¿‡ç¼©å†™è¯ï¼Œé¿å…å¦‚ "that's" è¢«é”™è¯¯åˆ†å‰²
                if i + 1 < len(doc) and doc[i + 1].text in ["'s", "'re", "'ve", "'ll", "'d"]:
                    continue
                
                # æ£€æŸ¥ä¸Šä¸‹æ–‡å•è¯æ•°é‡ï¼Œç¡®ä¿åˆ†å‰²ç‚¹ä¸¤ä¾§éƒ½æœ‰è¶³å¤Ÿçš„å†…å®¹
                left_words = [word for word in doc[max(0, token.i - context_words):token.i] if not word.is_punct]
                right_words = [word for word in doc[token.i+1:min(len(doc), token.i + context_words + 1)] if not word.is_punct]
                
                if len(left_words) >= context_words and len(right_words) >= context_words and should_split:
                    rprint(f"[yellow]âœ‚ï¸  åœ¨è¿æ¥è¯å‰åˆ†å‰² '{token.text}': {' '.join(t.text for t in left_words)} | {token.text} {' '.join(t.text for t in right_words)}[/yellow]")
                    new_sentences.append(doc[start:token.i].text.strip())  # æ·»åŠ åˆ†å‰²ç‚¹å‰çš„éƒ¨åˆ†
                    start = token.i  # æ›´æ–°ä¸‹ä¸€å¥çš„èµ·å§‹ç‚¹
                    split_occurred = True
                    break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹åï¼Œç«‹å³è·³å‡ºå†…å±‚å¾ªç¯ï¼Œå¤„ç†ä¸‹ä¸€ä¸ªå¥å­ç‰‡æ®µ
            
            # å°†å½“å‰å¤„ç†çš„å¥å­çš„å‰©ä½™éƒ¨åˆ†ï¼ˆæˆ–æœªè¢«åˆ†å‰²çš„æ•´ä¸ªå¥å­ï¼‰æ·»åŠ åˆ°æ–°åˆ—è¡¨
            if start < len(doc):
                new_sentences.append(doc[start:].text.strip())
        
        # å¦‚æœæœ¬è½®è¿­ä»£æ²¡æœ‰å‘ç”Ÿä»»ä½•åˆ†å‰²ï¼Œè¯´æ˜å¤„ç†å®Œæˆï¼Œé€€å‡ºå¾ªç¯
        if not split_occurred:
            break
        
        # ç”¨æ–°ç”Ÿæˆçš„å¥å­åˆ—è¡¨æ›¿æ¢æ—§çš„ï¼Œå‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£
        sentences = new_sentences
    
    return sentences

def split_sentences_main(nlp):
    """
    æ‰§è¡Œè¿æ¥è¯åˆ†å‰²çš„ä¸»å‡½æ•°ï¼Œæ˜¯æ•´ä¸ªæµç¨‹çš„ä¸€éƒ¨åˆ†ã€‚
    å®ƒè¯»å–å‰ä¸€é˜¶æ®µçš„è¾“å‡ºï¼Œè¿›è¡Œè¿æ¥è¯åˆ†å‰²ï¼Œç„¶åå°†ç»“æœå†™å…¥æ–°æ–‡ä»¶ï¼Œå¹¶æ¸…ç†æ‰å‰ä¸€é˜¶æ®µçš„æ–‡ä»¶ã€‚

    Args:
        nlp (spacy.Language): å·²åŠ è½½çš„ spaCy æ¨¡å‹å®ä¾‹ã€‚
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ­¤æ­¥éª¤
    if not os.path.exists(SPLIT_BY_COMMA_FILE):
        rprint(f"[yellow]æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ {SPLIT_BY_COMMA_FILE}ï¼Œè·³è¿‡è¿æ¥è¯åˆ†å‰²æ­¥éª¤ã€‚[/yellow]")
        write_to_file(SPLIT_BY_CONNECTOR_FILE, "") # åˆ›å»ºç©ºæ–‡ä»¶ä»¥ä¿è¯æµç¨‹è¿è´¯
        return

    with open(SPLIT_BY_COMMA_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()
    
    all_split_sentences = []
    for sentence in sentences:
        # å¯¹æ¯ä¸ªä»æ–‡ä»¶ä¸­è¯»å‡ºçš„å¥å­åº”ç”¨è¿æ¥è¯åˆ†å‰²é€»è¾‘
        split_sentences = split_by_connectors(sentence.strip(), nlp=nlp)
        all_split_sentences.extend(split_sentences)
    
    # å°†æ‰€æœ‰å¤„ç†åçš„å¥å­è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨æ¢è¡Œç¬¦åˆ†éš”ï¼Œç„¶åä¸€æ¬¡æ€§å†™å…¥æ–‡ä»¶
    # è¿™æ ·æ›´é«˜æ•ˆï¼Œä¹Ÿé¿å…äº†åœ¨æ–‡ä»¶æœ«å°¾ç•™ä¸‹å¤šä½™çš„æ¢è¡Œç¬¦
    output_content = "\n".join(filter(None, all_split_sentences))
    write_to_file(SPLIT_BY_CONNECTOR_FILE, output_content)

    # é‡è¦ï¼šåˆ é™¤ä½œä¸ºè¾“å…¥çš„ä¸Šä¸€æ­¥ä¸­é—´æ–‡ä»¶ï¼Œè¿™æ˜¯æµæ°´çº¿è®¾è®¡çš„æ ¸å¿ƒéƒ¨åˆ†
    os.remove(SPLIT_BY_COMMA_FILE)
    
    rprint(f"[green]ğŸ’¾ æŒ‰è¿æ¥è¯åˆ†å‰²å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° â†’ `{SPLIT_BY_CONNECTOR_FILE}`[/green]")

if __name__ == "__main__":
    # --- æµ‹è¯•ä»£ç  ---
    test_nlp = init_nlp()
    test_sentence = "and show the specific differences that make a difference between a breakaway that results in a goal in the NHL versus one that doesn't."
    rprint(f"åŸå§‹å¥å­: {test_sentence}")
    split_result = split_by_connectors(test_sentence, nlp=test_nlp)
    rprint("--- åˆ†å‰²ç»“æœ ---")
    for s in split_result:
        print(s)