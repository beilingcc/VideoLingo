# -*- coding: utf-8 -*-
"""
åŸºäºæ ‡ç‚¹ç¬¦å·çš„å¥å­åˆ†å‰²æ¨¡å—ï¼ˆæµæ°´çº¿å…¥å£ï¼‰

æœ¬æ¨¡å—æ˜¯æ•´ä¸ªå¥å­åˆ†å‰²æµæ°´çº¿çš„èµ·å§‹ç‚¹ã€‚å®ƒè´Ÿè´£ä»ä¸€ä¸ªåŒ…å«åŸå§‹æ–‡æœ¬ç‰‡æ®µçš„ Excel æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œ
è¿›è¡Œåˆæ­¥çš„ã€åŸºäºæ ‡å‡†ç»“æŸæ ‡ç‚¹ï¼ˆå¦‚å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰çš„åˆ†å‰²ã€‚

æ ¸å¿ƒåŠŸèƒ½:
1.  **æ•°æ®è¯»å–ä¸æ‹¼æ¥**:
    - ä» `output/log/cleaned_chunks.xlsx` æ–‡ä»¶ä¸­è¯»å–ç”± ASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µã€‚
    - æ ¹æ®é¡¹ç›®é…ç½®çš„è¯­è¨€ï¼Œä½¿ç”¨ç‰¹å®šçš„è¿æ¥ç¬¦ï¼ˆ`joiner`ï¼‰å°†è¿™äº›æ–‡æœ¬ç‰‡æ®µæ™ºèƒ½åœ°æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„é•¿æ–‡æœ¬ã€‚
      ä¾‹å¦‚ï¼Œè‹±æ–‡ä½¿ç”¨ç©ºæ ¼è¿æ¥ï¼Œè€Œä¸­æ–‡ã€æ—¥æ–‡åˆ™ç›´æ¥æ‹¼æ¥ã€‚

2.  **åŸºç¡€åˆ†å‰²**:
    - åˆ©ç”¨ spaCy å¼ºå¤§çš„ `doc.sents` åŠŸèƒ½ï¼Œå¯¹æ‹¼æ¥åçš„é•¿æ–‡æœ¬è¿›è¡Œç¬¬ä¸€æ¬¡åˆ†å‰²ã€‚è¿™ä¸€æ­¥ä¸»è¦ä¾èµ–äº
      æ¨¡å‹å¯¹æ ‡å‡†å¥å­ç»“æŸæ ‡ç‚¹çš„è¯†åˆ«ã€‚

3.  **ç‰¹æ®Šæƒ…å†µåˆå¹¶**:
    - **å¤„ç†ç ´æŠ˜å·å’Œçœç•¥å·**: åŒ…å«ä¸“é—¨çš„é€»è¾‘æ¥è¯†åˆ«å¹¶åˆå¹¶é‚£äº›è¢« `-` (ç ´æŠ˜å·) æˆ– `...` (çœç•¥å·)
      é”™è¯¯æ–­å¼€çš„å¥å­ï¼Œç¡®ä¿è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚
    - **ä¿®æ­£æ‚¬æŒ‚æ ‡ç‚¹**: è§£å†³åœ¨æŸäº›è¯­è¨€ï¼ˆç‰¹åˆ«æ˜¯ä¸­æ—¥éŸ©è¯­è¨€ï¼‰ä¸­ï¼Œåˆ†å‰²åå¯èƒ½äº§ç”ŸåªåŒ…å«å•ä¸ªæ ‡ç‚¹çš„è¡Œçš„é—®é¢˜ã€‚
      å®ƒä¼šå°†è¿™äº›ç‹¬ç«‹çš„æ ‡ç‚¹ç¬¦å·åˆå¹¶åˆ°å‰ä¸€ä¸ªå¥å­çš„æœ«å°¾ã€‚

4.  **è¾“å‡ºä¸è¡”æ¥**:
    - å°†å¤„ç†å¥½çš„å¥å­åˆ—è¡¨å†™å…¥åˆ° `SPLIT_BY_MARK_FILE` æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œä¸€ä¸ªå¥å­ã€‚
    - è¿™ä¸ªè¾“å‡ºæ–‡ä»¶å°†ä½œä¸ºåç»­åˆ†å‰²æ­¥éª¤ï¼ˆå¦‚ `split_by_comma.py`ï¼‰çš„è¾“å…¥ï¼Œä»è€Œå¯åŠ¨æ•´ä¸ªåˆ†å‰²æµæ°´çº¿ã€‚
"""

import os
import pandas as pd
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_MARK_FILE
from core.utils.config_utils import load_key, get_joiner
from core.utils import rprint, write_to_file

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore", category=FutureWarning)

def split_by_mark(nlp):
    """
    æ‰§è¡ŒåŸºäºæ ‡ç‚¹çš„å¥å­åˆ†å‰²ï¼Œå¹¶å¤„ç†ç‰¹æ®Šçš„åˆå¹¶é€»è¾‘ã€‚

    Args:
        nlp (spacy.Language): å·²åŠ è½½çš„ spaCy æ¨¡å‹å®ä¾‹ã€‚
    """
    # --- 1. åŠ è½½é…ç½®å¹¶æ‹¼æ¥åŸå§‹æ–‡æœ¬ ---
    whisper_language = load_key("whisper.language")
    # ä¼˜å…ˆä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€ï¼ŒåŒæ—¶å…¼å®¹å¼ºåˆ¶æŒ‡å®šè¯­è¨€çš„æƒ…å†µ
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language
    joiner = get_joiner(language)
    rprint(f"[blue]ğŸ” ä½¿ç”¨ '{language}' è¯­è¨€è¿æ¥ç¬¦: '{joiner}'[/blue]")
    
    # ä» Excel æ–‡ä»¶è¯»å–åŸå§‹æ–‡æœ¬å—
    chunks = pd.read_excel("output/log/cleaned_chunks.xlsx")
    # æ¸…ç†æ¯ä¸ªæ–‡æœ¬å—é¦–å°¾å¯èƒ½å­˜åœ¨çš„æ— å…³å¼•å·
    chunks.text = chunks.text.apply(lambda x: x.strip('"').strip())
    
    # ä½¿ç”¨ç‰¹å®šè¯­è¨€çš„è¿æ¥ç¬¦å°†æ‰€æœ‰æ–‡æœ¬å—æ‹¼æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²
    input_text = joiner.join(chunks.text.to_list())

    # --- 2. ä½¿ç”¨ spaCy è¿›è¡ŒåŸºç¡€åˆ†å‰² ---
    doc = nlp(input_text)
    assert doc.has_annotation("SENT_START"), "spaCyæœªèƒ½æˆåŠŸè¿›è¡Œå¥å­åˆ†å‰²ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ã€‚"

    # --- 3. å¤„ç†ç‰¹æ®Šåˆå¹¶é€»è¾‘ï¼ˆç ´æŠ˜å·å’Œçœç•¥å·ï¼‰ ---
    # è¿™ä¸ªåˆ—è¡¨ç”¨äºå­˜å‚¨åˆæ­¥åˆ†å‰²å¹¶ç»è¿‡åˆå¹¶å¤„ç†çš„å¥å­
    sentences_after_merge = []
    current_sentence_parts = []
    
    for sent in doc.sents:
        text = sent.text.strip()
        if not text:
            continue

        # åˆ¤æ–­å½“å‰å¥å­æ˜¯å¦åº”ä¸å‰ä¸€éƒ¨åˆ†åˆå¹¶ï¼ˆä¾‹å¦‚ï¼Œä»¥ '...' æˆ– '-' å¼€å§‹/ç»“æŸï¼‰
        if current_sentence_parts and (
            text.startswith('-') or 
            text.startswith('...') or
            current_sentence_parts[-1].endswith('-') or
            current_sentence_parts[-1].endswith('...')
        ):
            current_sentence_parts.append(text)
        else:
            # å¦‚æœä¸éœ€åˆå¹¶ï¼Œåˆ™å°†ä¹‹å‰ç¼“å­˜çš„éƒ¨åˆ†è¿æ¥æˆä¸€ä¸ªå®Œæ•´å¥å­
            if current_sentence_parts:
                sentences_after_merge.append(' '.join(current_sentence_parts))
            # å¼€å§‹ä¸€ä¸ªæ–°çš„å¥å­ç‰‡æ®µ
            current_sentence_parts = [text]
    
    # å°†æœ€åä¸€ä¸ªç¼“å­˜çš„å¥å­æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
    if current_sentence_parts:
        sentences_after_merge.append(' '.join(current_sentence_parts))

    # --- 4. å¤„ç†æ‚¬æŒ‚æ ‡ç‚¹å¹¶å‡†å¤‡æœ€ç»ˆè¾“å‡º ---
    final_sentences = []
    punctuation_to_merge = [',', '.', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼']
    for i, sentence in enumerate(sentences_after_merge):
        # å¦‚æœå½“å‰è¡ŒåªåŒ…å«ä¸€ä¸ªæ ‡ç‚¹ï¼Œå¹¶ä¸”ä¸æ˜¯ç¬¬ä¸€è¡Œï¼Œåˆ™å°†å…¶é™„åŠ åˆ°å‰ä¸€è¡Œçš„æœ«å°¾
        if i > 0 and sentence.strip() in punctuation_to_merge and final_sentences:
            final_sentences[-1] += sentence
        else:
            final_sentences.append(sentence)

    # --- 5. å†™å…¥æ–‡ä»¶ ---
    output_content = "\n".join(filter(None, final_sentences))
    write_to_file(SPLIT_BY_MARK_FILE, output_content)
    
    rprint(f"[green]ğŸ’¾ å·²æŒ‰æ ‡ç‚¹åˆ†å‰²å¥å­ï¼Œç»“æœä¿å­˜è‡³ â†’ `{SPLIT_BY_MARK_FILE}`[/green]")

if __name__ == "__main__":
    nlp_instance = init_nlp()
    split_by_mark(nlp_instance)
