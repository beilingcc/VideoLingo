# -*- coding: utf-8 -*-
"""
åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„è¯­ä¹‰åˆ†å‰²æ¨¡å—

æœ¬æ¨¡å—æ˜¯ç»§åŸºäºè§„åˆ™çš„ NLP åˆ†å‰²ï¼ˆ_3_1_split_nlp.pyï¼‰ä¹‹åçš„è¿›é˜¶å¤„ç†æ­¥éª¤ã€‚
å®ƒçš„æ ¸å¿ƒä»»åŠ¡æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰çš„å¼ºå¤§è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œ
å¯¹é‚£äº›ä¾ç„¶è¿‡é•¿çš„å¥å­è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†ã€‚è¿™å¯¹äºå¤„ç†å¤æ‚å¥å¼ã€å£è¯­åŒ–è¡¨è¾¾ä»¥åŠ
æ²¡æœ‰æ˜æ˜¾è¯­æ³•åˆ†å‰²ç‚¹çš„æ–‡æœ¬è‡³å…³é‡è¦ã€‚

æ ¸å¿ƒåŠŸèƒ½:
- **æ™ºèƒ½è¯­ä¹‰åˆ†å‰²**: é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ Promptï¼Œå¼•å¯¼ LLM åœ¨ä¿æŒå¥å­æ ¸å¿ƒæ„ä¹‰ä¸æ–­è£‚çš„å‰æä¸‹ï¼Œ
  æ‰¾åˆ°æœ€åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œå¹¶æ’å…¥ç‰¹æ®Šæ ‡è®° `[br]`ã€‚
- **ç²¾ç¡®ä½ç½®æ˜ å°„**: LLM è¿”å›çš„åˆ†å‰²ç»“æœå¯èƒ½åœ¨æªè¾ä¸Šä¸åŸæ–‡æœ‰ç»†å¾®å·®å¼‚ã€‚
  æœ¬æ¨¡å—é€šè¿‡ `difflib.SequenceMatcher` ç®—æ³•ï¼Œå°† LLM å»ºè®®çš„åˆ†å‰²ç‚¹ç²¾ç¡®åœ°æ˜ å°„å›
  åŸå§‹å¥å­çš„å­—ç¬¦ä½ç½®ï¼Œç¡®ä¿åˆ†å‰²æ“ä½œä¸ä¼šä¿®æ”¹åŸæ–‡å†…å®¹ï¼Œåªåœ¨æ°å½“ä½ç½®æ’å…¥æ¢è¡Œç¬¦ã€‚
- **å¹¶è¡Œå¤„ç†**: å¯¹äºéœ€è¦å¤„ç†çš„å¤§é‡å¥å­ï¼Œé‡‡ç”¨å¤šçº¿ç¨‹ (`concurrent.futures.ThreadPoolExecutor`)
  å¹¶è¡Œè°ƒç”¨ LLM APIï¼Œæå¤§åœ°æé«˜äº†å¤„ç†æ•ˆç‡ã€‚
- **å¥å£®çš„é‡è¯•æœºåˆ¶**: è€ƒè™‘åˆ°ç½‘ç»œè¯·æ±‚å’Œ LLM å“åº”çš„ä¸ç¡®å®šæ€§ï¼Œå†…ç½®äº†é‡è¯•å¾ªç¯ï¼Œ
  ç¡®ä¿æ‰€æœ‰é•¿å¥éƒ½èƒ½è¢«æˆåŠŸå¤„ç†ã€‚

ä½¿ç”¨æ–¹æ³•:
  ä¸»å‡½æ•° `split_sentences_by_meaning()` ä¼šè¯»å–ä¸Šä¸€æ­¥ NLP åˆ†å‰²çš„ç»“æœï¼Œ
  ç­›é€‰å‡ºé•¿åº¦è¶…è¿‡é¢„è®¾é˜ˆå€¼çš„å¥å­ï¼Œç„¶åè°ƒç”¨å¹¶è¡Œå¤„ç†å‡½æ•° `parallel_split_sentences`
  è¿›è¡Œå¤„ç†ï¼Œæœ€åå°†å®Œå…¨åˆ†å‰²å¥½çš„å¥å­åˆ—è¡¨å†™å…¥æ–‡ä»¶ï¼Œä¾›åç»­æµç¨‹ä½¿ç”¨ã€‚
"""

import concurrent.futures
from difflib import SequenceMatcher
import math
from core.prompts import get_split_prompt
from core.spacy_utils.load_nlp_model import init_nlp
from core.utils import *
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress
from core.utils.models import _3_1_SPLIT_BY_NLP, _3_2_SPLIT_BY_MEANING

console = Console()

def tokenize_sentence(sentence: str, nlp) -> list:
    doc = nlp(sentence)
    return [token.text for token in doc]

def find_split_positions(original: str, modified: str) -> list:
    split_positions = []
    parts = modified.split('[br]')
    start = 0
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language
    joiner = get_joiner(language)

    for i in range(len(parts) - 1):
        max_similarity = 0
        best_split = None
        for j in range(start, len(original)):
            original_left = original[start:j]
            modified_left = joiner.join(parts[i].split())
            left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()
            if left_similarity > max_similarity:
                max_similarity = left_similarity
                best_split = j

        if max_similarity < 0.9:
            console.log(f"[yellow]ä½ç›¸ä¼¼åº¦è­¦å‘Š: {max_similarity:.2f} for part '{parts[i]}...'[/yellow]")
        if best_split is not None:
            split_positions.append(best_split)
            start = best_split
        else:
            console.log(f"[red]åˆ†å‰²ç‚¹æŸ¥æ‰¾å¤±è´¥ for part {i+1}[/red]")
    return split_positions

def split_sentence(sentence: str, num_parts: int, word_limit: int = 20, index: int = -1, retry_attempt: int = 0) -> str:
    split_prompt = get_split_prompt(sentence, num_parts, word_limit)
    def valid_split(response_data):
        choice = response_data.get("choice")
        if not choice or f'split{choice}' not in response_data:
            return {"status": "error", "message": "å“åº”ä¸­ç¼ºå°‘ 'split' æˆ– 'choice' é”®"}
        if "[br]" not in response_data[f"split{choice}"]:
            return {"status": "error", "message": "åˆ†å‰²å¤±è´¥ï¼Œæœªæ‰¾åˆ° [br] æ ‡è®°"}
        return {"status": "success", "message": "åˆ†å‰²æˆåŠŸ"}

    response_data = ask_gpt(split_prompt + " " * retry_attempt, resp_type='json', valid_def=valid_split, log_title='split_by_meaning')
    choice = response_data["choice"]
    best_split_modified = response_data[f"split{choice}"]
    split_points = find_split_positions(sentence, best_split_modified)

    result_parts = []
    last_pos = 0
    for pos in split_points:
        result_parts.append(sentence[last_pos:pos])
        last_pos = pos
    result_parts.append(sentence[last_pos:])
    best_split_original = '\n'.join(part.strip() for part in result_parts if part.strip())

    # ä»…åœ¨æ—¥å¿—æ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†è¡¨æ ¼
    # table = Table(title=f"å¥å­ {index} åˆ†å‰²è¯¦æƒ…", show_header=False, box=box.SIMPLE)
    # table.add_row("[cyan]åŸå§‹[/cyan]", sentence)
    # table.add_row("[green]åˆ†å‰²å[/green]", best_split_original.replace('\n', ' [color(248)]||[/color(248)] '))
    # console.log(table)
    
    return best_split_original

def parallel_split_sentences(sentences_to_split: list, max_length: int, max_workers: int, nlp, retry_attempt: int = 0) -> list:
    results = [None] * len(sentences_to_split)
    futures = []
    with Progress(console=console) as progress:
        task = progress.add_task(f"[cyan]  - ç¬¬ {retry_attempt + 1} è½®è¯­ä¹‰åˆ†å‰²...", total=len(sentences_to_split))
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for index, (original_index, sentence) in enumerate(sentences_to_split):
                tokens = tokenize_sentence(sentence, nlp)
                num_parts = math.ceil(len(tokens) / max_length)
                future = executor.submit(split_sentence, sentence, num_parts, max_length, index=original_index, retry_attempt=retry_attempt)
                futures.append((future, original_index, sentence))

            for future, original_index, sentence in futures:
                try:
                    split_result = future.result()
                    if split_result:
                        split_lines = split_result.strip().split('\n')
                        results[futures.index((future, original_index, sentence))] = [line.strip() for line in split_lines]
                    else:
                        results[futures.index((future, original_index, sentence))] = [sentence]
                except Exception as e:
                    console.log(f"[red]é”™è¯¯: åˆ†å‰²å¥å­ {original_index} æ—¶å‡ºé”™: {e}[/red]")
                    results[futures.index((future, original_index, sentence))] = [sentence]
                progress.update(task, advance=1)
    return results

@check_file_exists(_3_2_SPLIT_BY_MEANING)
def split_meaning_main():
    """
    æŒ‰è¯­ä¹‰åˆ†å‰²å¥å­çš„ä¸»å‡½æ•°ã€‚
    """
    console.print(Panel("[bold cyan]ğŸ§  å¯åŠ¨åŸºäº LLM çš„è¯­ä¹‰åˆ†å‰²æµç¨‹...[/bold cyan]", title="ç¬¬ä¸‰æ­¥ (è¡¥å……): è¯­ä¹‰åˆ†å‰²", expand=False))
    try:
        # æ­¥éª¤ 1: åŠ è½½ä¸Šä¸€æ­¥çš„åˆ†å‰²ç»“æœ
        console.print(f"[cyan]- æ­¥éª¤ 1/4: æ­£åœ¨åŠ è½½ NLP åˆ†å‰²åçš„å¥å­...[/cyan]")
        try:
            with open(_3_1_SPLIT_BY_NLP, 'r', encoding='utf-8') as f:
                sentences = [line.strip() for line in f.readlines() if line.strip()]
            console.print(f"[green]  âœ… æˆåŠŸåŠ è½½ {len(sentences)} ä¸ªå¥å­ï¼Œæºæ–‡ä»¶: `{_3_1_SPLIT_BY_NLP}`[/green]")
        except FileNotFoundError:
            console.print(Panel(f"[bold red]âŒ è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°[/bold red]", subtitle=f"è¯·ç¡®ä¿ `{_3_1_SPLIT_BY_NLP}` æ–‡ä»¶å­˜åœ¨ã€‚", expand=False))
            return

        # æ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹å’Œé…ç½®
        console.print(f"[cyan]- æ­¥éª¤ 2/4: æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œé…ç½®...[/cyan]")
        nlp = init_nlp()
        max_len = load_key("max_split_length")
        max_w = load_key("max_workers")
        console.print(f"[green]  âœ… æ¨¡å‹å’Œé…ç½®åŠ è½½å®Œæ¯• (æœ€å¤§é•¿åº¦: {max_len}, æœ€å¤§çº¿ç¨‹æ•°: {max_w})ã€‚[/green]")

        # æ­¥éª¤ 3: å¾ªç¯è¿›è¡Œå¹¶è¡Œè¯­ä¹‰åˆ†å‰²
        console.print(f"[cyan]- æ­¥éª¤ 3/4: æ­£åœ¨å¯¹é•¿å¥è¿›è¡Œå¤šè½®å¹¶è¡Œè¯­ä¹‰åˆ†å‰²...[/cyan]")
        for retry_attempt in range(3): # æœ€å¤šè¿›è¡Œ3è½®åˆ†å‰²
            long_sentences_to_split = [(i, s) for i, s in enumerate(sentences) if len(tokenize_sentence(s, nlp)) > max_len]
            
            if not long_sentences_to_split:
                console.print("[green]  âœ… æ‰€æœ‰å¥å­é•¿åº¦å‡å·²è¾¾æ ‡ï¼Œæ— éœ€è¿›ä¸€æ­¥åˆ†å‰²ã€‚[/green]")
                break

            console.print(f"[cyan]  - ç¬¬ {retry_attempt + 1} è½®åˆ†å‰²å¼€å§‹ (å‘ç° {len(long_sentences_to_split)} ä¸ªé•¿å¥)...[/cyan]")
            split_results = parallel_split_sentences([(i, s) for i, s in long_sentences_to_split], max_length=max_len, max_workers=max_w, nlp=nlp, retry_attempt=retry_attempt)
            
            # å°†åˆ†å‰²ç»“æœæ›´æ–°å›ä¸»åˆ—è¡¨
            new_sentences = list(sentences)
            for (original_index, _), result_lines in zip(long_sentences_to_split, split_results):
                new_sentences[original_index] = result_lines
            
            sentences = [item for sublist in new_sentences for item in (sublist if isinstance(sublist, list) else [sublist])]

            needs_another_round = any(len(tokenize_sentence(s, nlp)) > max_len for s in sentences)
            if not needs_another_round:
                console.print(f"[green]  âœ… ç¬¬ {retry_attempt + 1} è½®åï¼Œæ‰€æœ‰å¥å­é•¿åº¦å‡å·²è¾¾æ ‡ã€‚[/green]")
                break
        console.print("[green]  âœ… è¯­ä¹‰åˆ†å‰²å¤„ç†å®Œæˆã€‚[/green]")

        # æ­¥éª¤ 4: ä¿å­˜æœ€ç»ˆç»“æœ
        console.print(f"[cyan]- æ­¥éª¤ 4/4: æ­£åœ¨ä¿å­˜æœ€ç»ˆç»“æœ...[/cyan]")
        with open(_3_2_SPLIT_BY_MEANING, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sentences))
        
        console.print(Panel(f"[bold green]ğŸ‰ è¯­ä¹‰åˆ†å‰²æµç¨‹å®Œæˆï¼[/bold green]", subtitle=f"ç»“æœå·²ä¿å­˜åˆ° `{_3_2_SPLIT_BY_MEANING}`", expand=False))

    except Exception as e:
        console.print(Panel(f"[bold red]âŒ è¯­ä¹‰åˆ†å‰²æµç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯[/bold red]", subtitle=str(e), expand=False))

if __name__ == '__main__':
    split_meaning_main()